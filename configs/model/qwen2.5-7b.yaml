name: "Qwen/Qwen2.5-7B"
type: "causal_lm"
architecture: "qwen2"

model_params:
  hidden_size: 3584
  num_hidden_layers: 28
  num_attention_heads: 28
  num_key_value_heads: 4
  intermediate_size: 18944
  vocab_size: 151936
  max_position_embeddings: 32768

load_in_8bit: false
load_in_4bit: false
torch_dtype: "bfloat16"
attn_implementation: "flash_attention_2"
